# Data Mart de Marketing Bancario - dbt Cloud

Este proyecto implementa un Data Mart completo para an√°lisis de campa√±as de marketing directo bancario utilizando **dbt Cloud** y **BigQuery**, con gobernanza opcional en **Dataplex**.

## üìä Descripci√≥n del Proyecto

El Data Mart procesa datos de campa√±as de marketing directo de una instituci√≥n bancaria portuguesa, transformando datos raw en insights accionables para el negocio.

### Datasets Procesados
- **Dataset Principal**: Informaci√≥n de clientes y campa√±as (4,521 registros)
- **Dataset Adicional**: Contexto socioecon√≥mico adicional (4,119 registros)

### M√©tricas Clave Implementadas
1. **Tasa de Conversi√≥n**: `contactos_exitosos / total_contactos`
2. **Segmentaci√≥n de Clientes**: Por edad, ocupaci√≥n y riesgo
3. **M√©tricas de Campa√±a**: Duraci√≥n, frecuencia, timing
4. **Perfiles de Riesgo**: Clasificaci√≥n autom√°tica de riesgo

## üèóÔ∏è Arquitectura del Proyecto

### Estructura de Carpetas
```
bank_marketing_dm/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ staging/           # Modelos de limpieza y estandarizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ intermediate/      # Modelos intermedios y transformaciones
‚îÇ   ‚îî‚îÄ‚îÄ marts/
‚îÇ       ‚îî‚îÄ‚îÄ marketing/     # Data marts finales
‚îú‚îÄ‚îÄ tests/                 # Pruebas personalizadas
‚îú‚îÄ‚îÄ macros/                # Macros de monitoreo y calidad
‚îú‚îÄ‚îÄ docs/                  # Documentaci√≥n t√©cnica
‚îú‚îÄ‚îÄ scripts/               # Scripts de integraci√≥n
‚îî‚îÄ‚îÄ dbt_project.yml        # Configuraci√≥n del proyecto
```

### Capas de Datos
1. **Staging (Vistas)**
   - `stg_bank_marketing`: Limpieza del dataset principal
   - `stg_bank_additional`: Limpieza del dataset adicional

2. **Intermediate (Tablas)**
   - `int_customer_profiles`: Perfiles y segmentaci√≥n de clientes
   - `int_campaign_performance`: M√©tricas de rendimiento de campa√±as

3. **Marts (Tablas)**
   - `fct_marketing_conversions`: Tabla de hechos principal
   - `dim_customer_segments`: Dimensi√≥n de segmentos

## üîÑ Proceso Seguido y Decisiones Tomadas

### 1. Uso de dbt Cloud
**Decisi√≥n**: Uso de dbt Cloud para mejor gesti√≥n y escalabilidad.

**Proceso**:
- Configuraci√≥n de jobs en dbt Cloud
- Adaptaci√≥n de configuraci√≥n para entorno cloud

### 2. Estrategia de Ingesta de Datos
**Decisi√≥n**: Carga directa en BigQuery.

**Razones**:
- Mejor rendimiento para datasets grandes
- Separaci√≥n clara entre datos raw y transformados
- Facilita la gobernanza de datos

**Proceso**:
- Eliminaci√≥n de archivos CSV del repositorio
- Creaci√≥n de datasets raw en BigQuery
- Uso de `dbt source()` para referenciar datos

### 3. Optimizaci√≥n de Tests
**Decisi√≥n**: Simplificar tests removiendo dependencias problem√°ticas.

**Proceso**:
- Eliminaci√≥n de `dbt_utils` por problemas de compatibilidad
- Implementaci√≥n de tests b√°sicos y confiables
- Uso de `not_null`, `unique`, `accepted_values`

### 4. Configuraci√≥n de BigQuery
**Decisi√≥n**: Usar datasets con prefijos autom√°ticos de dbt Cloud.

**Proceso**:
- Configuraci√≥n de location `us-central1`
- Naming convention: `bank_marketing_dev_{layer}`
- Creaci√≥n autom√°tica de datasets

## üöÄ Configuraci√≥n y Ejecuci√≥n

### Prerrequisitos
- Proyecto de **GCP** con **BigQuery** habilitado
- **Service Account** con permisos (BigQuery Admin/Data Editor, Job User)
- Repositorio Git (GitHub, GitLab, Bitbucket)
- Acceso a **dbt Cloud**

### Paso 1: Preparar Datos en BigQuery

#### 1.1 Crear Datasets
```sql
-- Ejecutar en BigQuery Console
CREATE SCHEMA IF NOT EXISTS `dusa-prj-sandbox.raw`
OPTIONS(location="us-central1", description="Datos raw del proyecto");

CREATE SCHEMA IF NOT EXISTS `dusa-prj-sandbox.bank_marketing_dev_staging`
OPTIONS(location="us-central1", description="Dataset para staging");

CREATE SCHEMA IF NOT EXISTS `dusa-prj-sandbox.bank_marketing_dev_intermediate`
OPTIONS(location="us-central1", description="Dataset para intermediate");

CREATE SCHEMA IF NOT EXISTS `dusa-prj-sandbox.bank_marketing_dev_marts`
OPTIONS(location="us-central1", description="Dataset para marts");
```

#### 1.2 Cargar Datos Raw
```bash
# Cargar dataset principal
bq load --location=us-central1 --source_format=CSV \
  dusa-prj-sandbox:raw.bank_marketing \
  bank-full.csv \
  age:INTEGER,job:STRING,marital:STRING,education:STRING,default:STRING,balance:INTEGER,housing:STRING,loan:STRING,contact:STRING,day:INTEGER,month:STRING,duration:INTEGER,campaign:INTEGER,pdays:INTEGER,previous:INTEGER,poutcome:STRING,y:STRING

# Cargar dataset adicional
bq load --location=Uus-central1 --source_format=CSV \
  dusa-prj-sandbox:raw.bank_additional \
  bank-additional-full.csv \
  age:INTEGER,job:STRING,marital:STRING,education:STRING,default:STRING,balance:INTEGER,housing:STRING,loan:STRING,contact:STRING,day:INTEGER,month:STRING,duration:INTEGER,campaign:INTEGER,pdays:INTEGER,previous:INTEGER,poutcome:STRING,y:STRING,emp_var_rate:FLOAT,cons_price_idx:FLOAT,cons_conf_idx:FLOAT,euribor3m:FLOAT,nr_employed:FLOAT
```

### Paso 2: Configurar dbt Cloud

#### 2.1 Crear Proyecto
1. Ir a [dbt Cloud](https://cloud.getdbt.com)
2. Crear nuevo proyecto
3. Conectar repositorio Git
4. Configurar conexi√≥n BigQuery:
   - **Project ID**: `dusa-prj-sandbox`
   - **Location**: `us-central1`
   - **Dataset**: `bank_marketing_dev`

#### 2.2 Configurar Jobs
Crear los siguientes jobs en dbt Cloud:

**Job 1: Build Completo**
```bash
dbt deps
dbt build --full-refresh
```
- **Schedule**: `0 2 * * *` (Diario a las 2 AM)
- **Timeout**: 3600 segundos

**Job 2: Documentaci√≥n**
```bash
dbt deps
dbt docs generate
dbt docs serve --port 8080
```
- **Schedule**: `0 4 * * 0` (Semanal los domingos)

### Paso 3: Ejecutar Pipeline

#### 3.1 Primera Ejecuci√≥n
```bash
# En dbt Cloud Development Environment
dbt deps
dbt build
dbt test
```

#### 3.2 Verificar Resultados
```sql
-- Verificar datos en staging
SELECT COUNT(*) FROM `dusa-prj-sandbox.bank_marketing_dev_staging.stg_bank_marketing`;

-- Verificar m√©tricas de conversi√≥n
SELECT 
    AVG(conversion_rate) as avg_conversion_rate,
    COUNT(*) as total_contacts
FROM `dusa-prj-sandbox.bank_marketing_dev_marts.fct_marketing_conversions`;
```

## üìà Consultas de Ejemplo

### An√°lisis de Conversi√≥n por Segmento
```sql
SELECT 
    cs.customer_segment,
    cs.segment_description,
    AVG(fmc.conversion_rate) as avg_conversion_rate,
    COUNT(*) as total_contacts
FROM `dusa-prj-sandbox.bank_marketing_dev_marts.fct_marketing_conversions` fmc
JOIN `dusa-prj-sandbox.bank_marketing_dev_intermediate.int_customer_profiles` icp 
    ON fmc.contact_id = icp.contact_id
JOIN `dusa-prj-sandbox.bank_marketing_dev_marts.dim_customer_segments` cs 
    ON icp.customer_segment = cs.customer_segment
GROUP BY cs.customer_segment, cs.segment_description
ORDER BY avg_conversion_rate DESC;
```

### Rendimiento de Campa√±as por Duraci√≥n
```sql
SELECT 
    icp.call_duration_category,
    AVG(fmc.conversion_rate) as conversion_rate,
    COUNT(*) as total_calls
FROM `dusa-prj-sandbox.bank_marketing_dev_marts.fct_marketing_conversions` fmc
JOIN `dusa-prj-sandbox.bank_marketing_dev_intermediate.int_campaign_performance` icp 
    ON fmc.contact_id = icp.contact_id
GROUP BY icp.call_duration_category
ORDER BY conversion_rate DESC;
```

## üß™ Tests y Validaci√≥n

### Tests Implementados
- **Unicidad**: `contact_id` en todas las tablas
- **Completitud**: Campos obligatorios con `not_null`
- **Valores v√°lidos**: `accepted_values` para categor√≠as
- **Distribuci√≥n**: Test personalizado para grupos de edad

### Ejecutar Tests
```bash
# Ejecutar todos los tests
dbt test

# Ejecutar test espec√≠fico
dbt test --models stg_bank_marketing

# Ejecutar test personalizado
dbt test --select test_age_group_distribution
```

## üìä Monitoreo y Calidad

### Macros de Monitoreo
El proyecto incluye macros para monitoreo autom√°tico:

```sql
-- Monitoreo de calidad
{{ quality_monitoring() }}

-- Alertas de calidad
{{ quality_alerts() }}

### M√©tricas a Monitorear
1. **Calidad de Datos**: Tasa de completitud > 95%
2. **Rendimiento**: Tiempo de build < 30 minutos
3. **Negocio**: Tasa de conversi√≥n > 10%

## üîí Gobernanza con Dataplex (Opcional)

### Configuraci√≥n
1. Habilitar Dataplex API en GCP
2. Ejecutar `scripts/dataplex_integration.sql`
3. Configurar pol√≠ticas de calidad autom√°ticas

### Beneficios
- Clasificaci√≥n autom√°tica de datos
- Pol√≠ticas de calidad en tiempo real
- Auditor√≠a de acceso y uso

## üìö Documentaci√≥n Adicional

- **`docs/CONFIGURACION_FINAL.md`**: Gu√≠a completa de configuraci√≥n
- **`docs/models.md`**: Documentaci√≥n t√©cnica de modelos
- **`CONTRIBUTING.md`**: Gu√≠as de contribuci√≥n

## üõ†Ô∏è Troubleshooting

### Problemas Comunes

**Error: "Dataset not found"**
```bash
# Verificar datasets creados
bq ls dusa-prj-sandbox
```

**Error: "Permission denied"**
```bash
# Verificar permisos de Service Account
gcloud projects get-iam-policy dusa-prj-sandbox
```

**Error: "Job timeout"**
- Aumentar timeout en configuraci√≥n de job
- Optimizar queries complejas
- Usar incremental models

### Logs y Debugging
- Ver logs en dbt Cloud: Jobs ‚Üí Job History
- Debugging local: `dbt debug`

## üìù Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles.

---

**Nota**: Este proyecto es un ejercicio t√©cnico basado en el dataset de UCI Bank Marketing. Los datos son simulados y no representan informaci√≥n real de clientes bancarios.
